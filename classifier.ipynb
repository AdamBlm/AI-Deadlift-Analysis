{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/amine.chraibi/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.io import read_video\n",
    "\n",
    "import open_clip\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeadliftVideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, transform=None, augmentation=None, frames_per_video=16):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.augmentation = augmentation\n",
    "        self.frames_per_video = frames_per_video\n",
    "        self.label_to_idx = {\n",
    "            'bad movement': 0,\n",
    "            'good movement': 1\n",
    "        }\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.label_to_idx[self.labels[idx]]\n",
    "        \n",
    "        # Read video frames\n",
    "        video_frames, _, info = read_video(video_path, pts_unit='sec')\n",
    "        total_frames = video_frames.shape[0]\n",
    "\n",
    "        # Sample frames evenly throughout the video\n",
    "        indices = torch.linspace(0, total_frames - 1, self.frames_per_video).long()\n",
    "        sampled_frames = video_frames[indices]\n",
    "        \n",
    "        # Apply transforms to frames\n",
    "        frames = []\n",
    "        for frame in sampled_frames:\n",
    "            frame = frame.permute(2, 0, 1)  # Convert to (C, H, W)\n",
    "            if self.augmentation:\n",
    "                frame = self.augmentation(frame)\n",
    "            if self.transform:\n",
    "                frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "    \n",
    "        # Stack frames into a tensor\n",
    "        frames_tensor = torch.stack(frames)  # Shape: [frames_per_video, C, H, W]\n",
    "    \n",
    "        return frames_tensor, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "    (patch_dropout): Identity()\n",
       "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): ModuleList(\n",
       "        (0-11): 12 x ResidualAttentionBlock(\n",
       "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_1): Identity()\n",
       "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (ls_2): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): ModuleList(\n",
       "      (0-11): 12 x ResidualAttentionBlock(\n",
       "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_1): Identity()\n",
       "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        )\n",
       "        (ls_2): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 512)\n",
       "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = 'ViT-B-32'  # You can choose other architectures\n",
    "pretrained = 'openai'\n",
    "\n",
    "model, _, _ = open_clip.create_model_and_transforms(\n",
    "    model_name=model_name,\n",
    "    pretrained=pretrained\n",
    ")\n",
    "model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCLIPClassifier(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(VideoCLIPClassifier, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.num_classes = num_classes\n",
    "        self.fc = nn.Linear(clip_model.visual.output_dim, num_classes)\n",
    "        \n",
    "    def forward(self, frames):\n",
    "        # frames: [batch_size, frames_per_video, C, H, W]\n",
    "        batch_size, frames_per_video, C, H, W = frames.shape\n",
    "        frames = frames.view(-1, C, H, W)  # Flatten frames\n",
    "        with torch.no_grad():\n",
    "            frame_features = self.clip_model.encode_image(frames)  # [batch_size * frames_per_video, output_dim]\n",
    "        frame_features = frame_features.view(batch_size, frames_per_video, -1)\n",
    "        video_features = frame_features.mean(dim=1)  # Average over frames\n",
    "        logits = self.fc(video_features)  # [batch_size, num_classes]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCLIPClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, clip_model, num_classes):\n",
    "        super(VideoCLIPClassifierWithAttention, self).__init__()\n",
    "        self.clip_model = clip_model\n",
    "        self.num_classes = num_classes\n",
    "        self.attention_layer = nn.MultiheadAttention(embed_dim=clip_model.visual.output_dim, num_heads=4)\n",
    "        self.fc = nn.Linear(clip_model.visual.output_dim, num_classes)\n",
    "    \n",
    "    def forward(self, frames):\n",
    "        batch_size, frames_per_video, C, H, W = frames.shape\n",
    "        frames = frames.view(-1, C, H, W)  # Flatten frames\n",
    "        with torch.no_grad():\n",
    "            frame_features = self.clip_model.encode_image(frames)\n",
    "        frame_features = frame_features.view(batch_size, frames_per_video, -1)  # [batch_size, frames_per_video, output_dim]\n",
    "\n",
    "        # Attention over frames\n",
    "        frame_features = frame_features.permute(1, 0, 2)  # [frames_per_video, batch_size, output_dim]\n",
    "        attn_output, _ = self.attention_layer(frame_features, frame_features, frame_features)\n",
    "        video_features = attn_output.mean(dim=0)  # [batch_size, output_dim]\n",
    "        logits = self.fc(video_features)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2  # Number of movement labels\n",
    "classifier = VideoCLIPClassifierWithAttention(model, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video Paths: ['./dataset/Adam/bad/bad001.mp4', './dataset/Adam/bad/bad002.mp4', './dataset/Adam/bad/bad003.mp4', './dataset/Adam/bad/bad004.mp4', './dataset/Adam/bad/bad005.mp4', './dataset/Adam/bad/bad006.mp4', './dataset/Adam/bad/bad007.mp4', './dataset/Adam/bad/bad008.mp4', './dataset/Adam/bad/bad009.mp4', './dataset/Adam/bad/bad010.mp4', './dataset/Adam/bad/bad011.mp4', './dataset/Adam/bad/bad012.mp4', './dataset/Adam/bad/bad013.mp4', './dataset/Adam/bad/bad014.mp4', './dataset/Adam/bad/bad015.mp4', './dataset/Adam/bad/bad016.mp4', './dataset/Adam/bad/bad017.mp4', './dataset/Adam/bad/bad018.mp4', './dataset/Adam/bad/bad019.mp4', './dataset/Adam/good/good001.mp4', './dataset/Adam/good/good002.mp4', './dataset/Adam/good/good003.mp4', './dataset/Adam/good/good004.mp4', './dataset/Adam/good/good005.mp4', './dataset/Adam/good/good006.mp4', './dataset/Adam/good/good007.mp4', './dataset/Adam/good/good008.mp4', './dataset/Adam/good/good009.mp4', './dataset/Adam/good/good010.mp4', './dataset/Adam/good/good011.mp4', './dataset/Adam/good/good012.mp4', './dataset/Adam/good/good013.mp4', './dataset/Adam/good/good014.mp4', './dataset/Adam/good/good015.mp4', './dataset/Adam/good/good016.mp4', './dataset/Adam/good/good017.mp4', './dataset/Adam/good/good018.mp4', './dataset/Adam/good/good019.mp4', './dataset/Adam/good/good020.mp4']\n",
      "Labels: ['bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'bad movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement', 'good movement']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_root = './dataset/Adam'\n",
    "\n",
    "video_paths = []\n",
    "labels = []\n",
    "\n",
    "for label_folder in ['bad', 'good']:\n",
    "    label_path = os.path.join(dataset_root, label_folder)\n",
    "    \n",
    "    if not os.path.isdir(label_path):\n",
    "        print(f\"Folder '{label_folder}' does not exist in '{dataset_root}'\")\n",
    "        continue\n",
    "\n",
    "    label = 'bad movement' if label_folder == 'bad' else 'good movement'\n",
    "    \n",
    "    for video_file in os.listdir(label_path):\n",
    "        if video_file.endswith('.mp4'):  \n",
    "            video_paths.append(os.path.join(label_path, video_file))\n",
    "            labels.append(label)\n",
    "\n",
    "print(\"Video Paths:\", video_paths)\n",
    "print(\"Labels:\", labels)\n",
    "\n",
    "train_video_paths, test_video_paths, train_labels, test_labels = train_test_split(\n",
    "    video_paths, labels, test_size=0.2, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "train_augmentation = transforms.RandomApply([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "], p=0.7)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize(\n",
    "        mean=(0.48145466, 0.4578275, 0.40821073),\n",
    "        std=(0.26862954, 0.26130258, 0.27577711)\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = DeadliftVideoDataset(train_video_paths, train_labels, transform=transform, augmentation=train_augmentation)\n",
    "test_dataset = DeadliftVideoDataset(test_video_paths, test_labels, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(classifier.fc.parameters(), lr=1e-4)  # Only train the classification head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/amine.chraibi/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [0/18], Loss: 0.8316\n",
      "Epoch [1/100], Step [5/18], Loss: 0.6826\n",
      "Epoch [1/100], Step [10/18], Loss: 0.6250\n",
      "Epoch [1/100], Step [15/18], Loss: 0.7229\n",
      "Epoch [1/100], Loss: 0.7130\n",
      "Epoch [2/100], Step [0/18], Loss: 0.7153\n",
      "Epoch [2/100], Step [5/18], Loss: 0.6835\n",
      "Epoch [2/100], Step [10/18], Loss: 0.6880\n",
      "Epoch [2/100], Step [15/18], Loss: 0.7073\n",
      "Epoch [2/100], Loss: 0.6965\n",
      "Epoch [3/100], Step [0/18], Loss: 0.6886\n",
      "Epoch [3/100], Step [5/18], Loss: 0.6997\n",
      "Epoch [3/100], Step [10/18], Loss: 0.6392\n",
      "Epoch [3/100], Step [15/18], Loss: 0.6993\n",
      "Epoch [3/100], Loss: 0.6892\n",
      "Epoch [4/100], Step [0/18], Loss: 0.7012\n",
      "Epoch [4/100], Step [5/18], Loss: 0.6681\n",
      "Epoch [4/100], Step [10/18], Loss: 0.8279\n",
      "Epoch [4/100], Step [15/18], Loss: 0.6847\n",
      "Epoch [4/100], Loss: 0.6874\n",
      "Epoch [5/100], Step [0/18], Loss: 0.6862\n",
      "Epoch [5/100], Step [5/18], Loss: 0.6635\n",
      "Epoch [5/100], Step [10/18], Loss: 0.6553\n",
      "Epoch [5/100], Step [15/18], Loss: 0.6429\n",
      "Epoch [5/100], Loss: 0.6797\n",
      "Epoch [6/100], Step [0/18], Loss: 0.6977\n",
      "Epoch [6/100], Step [5/18], Loss: 0.6716\n",
      "Epoch [6/100], Step [10/18], Loss: 0.7337\n",
      "Epoch [6/100], Step [15/18], Loss: 0.7006\n",
      "Epoch [6/100], Loss: 0.6719\n",
      "Epoch [7/100], Step [0/18], Loss: 0.8029\n",
      "Epoch [7/100], Step [5/18], Loss: 0.6519\n",
      "Epoch [7/100], Step [10/18], Loss: 0.7424\n",
      "Epoch [7/100], Step [15/18], Loss: 0.6919\n",
      "Epoch [7/100], Loss: 0.6656\n",
      "Epoch [8/100], Step [0/18], Loss: 0.7418\n",
      "Epoch [8/100], Step [5/18], Loss: 0.6083\n",
      "Epoch [8/100], Step [10/18], Loss: 0.6551\n",
      "Epoch [8/100], Step [15/18], Loss: 0.6676\n",
      "Epoch [8/100], Loss: 0.6596\n",
      "Epoch [9/100], Step [0/18], Loss: 0.6305\n",
      "Epoch [9/100], Step [5/18], Loss: 0.6639\n",
      "Epoch [9/100], Step [10/18], Loss: 0.6469\n",
      "Epoch [9/100], Step [15/18], Loss: 0.6862\n",
      "Epoch [9/100], Loss: 0.6524\n",
      "Epoch [10/100], Step [0/18], Loss: 0.6695\n",
      "Epoch [10/100], Step [5/18], Loss: 0.6870\n",
      "Epoch [10/100], Step [10/18], Loss: 0.6439\n",
      "Epoch [10/100], Step [15/18], Loss: 0.7523\n",
      "Epoch [10/100], Loss: 0.6496\n",
      "Epoch [11/100], Step [0/18], Loss: 0.5832\n",
      "Epoch [11/100], Step [5/18], Loss: 0.6994\n",
      "Epoch [11/100], Step [10/18], Loss: 0.6433\n",
      "Epoch [11/100], Step [15/18], Loss: 0.6608\n",
      "Epoch [11/100], Loss: 0.6427\n",
      "Epoch [12/100], Step [0/18], Loss: 0.6478\n",
      "Epoch [12/100], Step [5/18], Loss: 0.7933\n",
      "Epoch [12/100], Step [10/18], Loss: 0.6422\n",
      "Epoch [12/100], Step [15/18], Loss: 0.6584\n",
      "Epoch [12/100], Loss: 0.6379\n",
      "Epoch [13/100], Step [0/18], Loss: 0.6327\n",
      "Epoch [13/100], Step [5/18], Loss: 0.6137\n",
      "Epoch [13/100], Step [10/18], Loss: 0.6426\n",
      "Epoch [13/100], Step [15/18], Loss: 0.6018\n",
      "Epoch [13/100], Loss: 0.6362\n",
      "Epoch [14/100], Step [0/18], Loss: 0.5254\n",
      "Epoch [14/100], Step [5/18], Loss: 0.5637\n",
      "Epoch [14/100], Step [10/18], Loss: 0.6015\n",
      "Epoch [14/100], Step [15/18], Loss: 0.5627\n",
      "Epoch [14/100], Loss: 0.6302\n",
      "Epoch [15/100], Step [0/18], Loss: 0.6154\n",
      "Epoch [15/100], Step [5/18], Loss: 0.5940\n",
      "Epoch [15/100], Step [10/18], Loss: 0.5437\n",
      "Epoch [15/100], Step [15/18], Loss: 0.5819\n",
      "Epoch [15/100], Loss: 0.6258\n",
      "Epoch [16/100], Step [0/18], Loss: 0.5357\n",
      "Epoch [16/100], Step [5/18], Loss: 0.6952\n",
      "Epoch [16/100], Step [10/18], Loss: 0.5694\n",
      "Epoch [16/100], Step [15/18], Loss: 0.5941\n",
      "Epoch [16/100], Loss: 0.6229\n",
      "Epoch [17/100], Step [0/18], Loss: 0.6362\n",
      "Epoch [17/100], Step [5/18], Loss: 0.6764\n",
      "Epoch [17/100], Step [10/18], Loss: 0.6927\n",
      "Epoch [17/100], Step [15/18], Loss: 0.6051\n",
      "Epoch [17/100], Loss: 0.6146\n",
      "Epoch [18/100], Step [0/18], Loss: 0.6163\n",
      "Epoch [18/100], Step [5/18], Loss: 0.6109\n",
      "Epoch [18/100], Step [10/18], Loss: 0.5374\n",
      "Epoch [18/100], Step [15/18], Loss: 0.5752\n",
      "Epoch [18/100], Loss: 0.6104\n",
      "Epoch [19/100], Step [0/18], Loss: 0.5427\n",
      "Epoch [19/100], Step [5/18], Loss: 0.5584\n",
      "Epoch [19/100], Step [10/18], Loss: 0.6058\n",
      "Epoch [19/100], Step [15/18], Loss: 0.6155\n",
      "Epoch [19/100], Loss: 0.6059\n",
      "Epoch [20/100], Step [0/18], Loss: 0.6848\n",
      "Epoch [20/100], Step [5/18], Loss: 0.6098\n",
      "Epoch [20/100], Step [10/18], Loss: 0.6243\n",
      "Epoch [20/100], Step [15/18], Loss: 0.6057\n",
      "Epoch [20/100], Loss: 0.6004\n",
      "Epoch [21/100], Step [0/18], Loss: 0.6423\n",
      "Epoch [21/100], Step [5/18], Loss: 0.6231\n",
      "Epoch [21/100], Step [10/18], Loss: 0.5274\n",
      "Epoch [21/100], Step [15/18], Loss: 0.6854\n",
      "Epoch [21/100], Loss: 0.5963\n",
      "Epoch [22/100], Step [0/18], Loss: 0.5612\n",
      "Epoch [22/100], Step [5/18], Loss: 0.6797\n",
      "Epoch [22/100], Step [10/18], Loss: 0.5220\n",
      "Epoch [22/100], Step [15/18], Loss: 0.6654\n",
      "Epoch [22/100], Loss: 0.5957\n",
      "Epoch [23/100], Step [0/18], Loss: 0.6297\n",
      "Epoch [23/100], Step [5/18], Loss: 0.6146\n",
      "Epoch [23/100], Step [10/18], Loss: 0.5230\n",
      "Epoch [23/100], Step [15/18], Loss: 0.5454\n",
      "Epoch [23/100], Loss: 0.5892\n",
      "Epoch [24/100], Step [0/18], Loss: 0.6299\n",
      "Epoch [24/100], Step [5/18], Loss: 0.5405\n",
      "Epoch [24/100], Step [10/18], Loss: 0.5809\n",
      "Epoch [24/100], Step [15/18], Loss: 0.6090\n",
      "Epoch [24/100], Loss: 0.5876\n",
      "Epoch [25/100], Step [0/18], Loss: 0.6313\n",
      "Epoch [25/100], Step [5/18], Loss: 0.5266\n",
      "Epoch [25/100], Step [10/18], Loss: 0.6150\n",
      "Epoch [25/100], Step [15/18], Loss: 0.5556\n",
      "Epoch [25/100], Loss: 0.5763\n",
      "Epoch [26/100], Step [0/18], Loss: 0.6265\n",
      "Epoch [26/100], Step [5/18], Loss: 0.5242\n",
      "Epoch [26/100], Step [10/18], Loss: 0.6056\n",
      "Epoch [26/100], Step [15/18], Loss: 0.6600\n",
      "Epoch [26/100], Loss: 0.5727\n",
      "Epoch [27/100], Step [0/18], Loss: 0.6546\n",
      "Epoch [27/100], Step [5/18], Loss: 0.5963\n",
      "Epoch [27/100], Step [10/18], Loss: 0.5040\n",
      "Epoch [27/100], Step [15/18], Loss: 0.5671\n",
      "Epoch [27/100], Loss: 0.5710\n",
      "Epoch [28/100], Step [0/18], Loss: 0.5498\n",
      "Epoch [28/100], Step [5/18], Loss: 0.6142\n",
      "Epoch [28/100], Step [10/18], Loss: 0.4735\n",
      "Epoch [28/100], Step [15/18], Loss: 0.5827\n",
      "Epoch [28/100], Loss: 0.5688\n",
      "Epoch [29/100], Step [0/18], Loss: 0.5212\n",
      "Epoch [29/100], Step [5/18], Loss: 0.4799\n",
      "Epoch [29/100], Step [10/18], Loss: 0.5282\n",
      "Epoch [29/100], Step [15/18], Loss: 0.5277\n",
      "Epoch [29/100], Loss: 0.5611\n",
      "Epoch [30/100], Step [0/18], Loss: 0.5339\n",
      "Epoch [30/100], Step [5/18], Loss: 0.5113\n",
      "Epoch [30/100], Step [10/18], Loss: 0.5840\n",
      "Epoch [30/100], Step [15/18], Loss: 0.4559\n",
      "Epoch [30/100], Loss: 0.5608\n",
      "Epoch [31/100], Step [0/18], Loss: 0.6134\n",
      "Epoch [31/100], Step [5/18], Loss: 0.5328\n",
      "Epoch [31/100], Step [10/18], Loss: 0.5654\n",
      "Epoch [31/100], Step [15/18], Loss: 0.5995\n",
      "Epoch [31/100], Loss: 0.5632\n",
      "Epoch [32/100], Step [0/18], Loss: 0.5383\n",
      "Epoch [32/100], Step [5/18], Loss: 0.7510\n",
      "Epoch [32/100], Step [10/18], Loss: 0.6653\n",
      "Epoch [32/100], Step [15/18], Loss: 0.6831\n",
      "Epoch [32/100], Loss: 0.5517\n",
      "Epoch [33/100], Step [0/18], Loss: 0.6042\n",
      "Epoch [33/100], Step [5/18], Loss: 0.6796\n",
      "Epoch [33/100], Step [10/18], Loss: 0.5071\n",
      "Epoch [33/100], Step [15/18], Loss: 0.5410\n",
      "Epoch [33/100], Loss: 0.5459\n",
      "Epoch [34/100], Step [0/18], Loss: 0.6978\n",
      "Epoch [34/100], Step [5/18], Loss: 0.6177\n",
      "Epoch [34/100], Step [10/18], Loss: 0.4782\n",
      "Epoch [34/100], Step [15/18], Loss: 0.4787\n",
      "Epoch [34/100], Loss: 0.5478\n",
      "Epoch [35/100], Step [0/18], Loss: 0.5153\n",
      "Epoch [35/100], Step [5/18], Loss: 0.4062\n",
      "Epoch [35/100], Step [10/18], Loss: 0.5161\n",
      "Epoch [35/100], Step [15/18], Loss: 0.8044\n",
      "Epoch [35/100], Loss: 0.5410\n",
      "Epoch [36/100], Step [0/18], Loss: 0.4842\n",
      "Epoch [36/100], Step [5/18], Loss: 0.4625\n",
      "Epoch [36/100], Step [10/18], Loss: 0.7022\n",
      "Epoch [36/100], Step [15/18], Loss: 0.4696\n",
      "Epoch [36/100], Loss: 0.5348\n",
      "Epoch [37/100], Step [0/18], Loss: 0.4887\n",
      "Epoch [37/100], Step [5/18], Loss: 0.6215\n",
      "Epoch [37/100], Step [10/18], Loss: 0.5782\n",
      "Epoch [37/100], Step [15/18], Loss: 0.4814\n",
      "Epoch [37/100], Loss: 0.5349\n",
      "Epoch [38/100], Step [0/18], Loss: 0.4176\n",
      "Epoch [38/100], Step [5/18], Loss: 0.4946\n",
      "Epoch [38/100], Step [10/18], Loss: 0.6024\n",
      "Epoch [38/100], Step [15/18], Loss: 0.4709\n",
      "Epoch [38/100], Loss: 0.5301\n",
      "Epoch [39/100], Step [0/18], Loss: 0.4557\n",
      "Epoch [39/100], Step [5/18], Loss: 0.4800\n",
      "Epoch [39/100], Step [10/18], Loss: 0.5012\n",
      "Epoch [39/100], Step [15/18], Loss: 0.4559\n",
      "Epoch [39/100], Loss: 0.5334\n",
      "Epoch [40/100], Step [0/18], Loss: 0.4276\n",
      "Epoch [40/100], Step [5/18], Loss: 0.4171\n",
      "Epoch [40/100], Step [10/18], Loss: 0.6118\n",
      "Epoch [40/100], Step [15/18], Loss: 0.3918\n",
      "Epoch [40/100], Loss: 0.5248\n",
      "Epoch [41/100], Step [0/18], Loss: 0.4914\n",
      "Epoch [41/100], Step [5/18], Loss: 0.7628\n",
      "Epoch [41/100], Step [10/18], Loss: 0.4523\n",
      "Epoch [41/100], Step [15/18], Loss: 0.5050\n",
      "Epoch [41/100], Loss: 0.5220\n",
      "Epoch [42/100], Step [0/18], Loss: 0.4761\n",
      "Epoch [42/100], Step [5/18], Loss: 0.5875\n",
      "Epoch [42/100], Step [10/18], Loss: 0.5401\n",
      "Epoch [42/100], Step [15/18], Loss: 0.4103\n",
      "Epoch [42/100], Loss: 0.5172\n",
      "Epoch [43/100], Step [0/18], Loss: 0.4052\n",
      "Epoch [43/100], Step [5/18], Loss: 0.5968\n",
      "Epoch [43/100], Step [10/18], Loss: 0.5789\n",
      "Epoch [43/100], Step [15/18], Loss: 0.4917\n",
      "Epoch [43/100], Loss: 0.5130\n",
      "Epoch [44/100], Step [0/18], Loss: 0.7213\n",
      "Epoch [44/100], Step [5/18], Loss: 0.7084\n",
      "Epoch [44/100], Step [10/18], Loss: 0.5050\n",
      "Epoch [44/100], Step [15/18], Loss: 0.4094\n",
      "Epoch [44/100], Loss: 0.5139\n",
      "Epoch [45/100], Step [0/18], Loss: 0.4612\n",
      "Epoch [45/100], Step [5/18], Loss: 0.5726\n",
      "Epoch [45/100], Step [10/18], Loss: 0.4600\n",
      "Epoch [45/100], Step [15/18], Loss: 0.4349\n",
      "Epoch [45/100], Loss: 0.5055\n",
      "Epoch [46/100], Step [0/18], Loss: 0.4448\n",
      "Epoch [46/100], Step [5/18], Loss: 0.6968\n",
      "Epoch [46/100], Step [10/18], Loss: 0.8303\n",
      "Epoch [46/100], Step [15/18], Loss: 0.4257\n",
      "Epoch [46/100], Loss: 0.5051\n",
      "Epoch [47/100], Step [0/18], Loss: 0.5871\n",
      "Epoch [47/100], Step [5/18], Loss: 0.7091\n",
      "Epoch [47/100], Step [10/18], Loss: 0.4298\n",
      "Epoch [47/100], Step [15/18], Loss: 0.6549\n",
      "Epoch [47/100], Loss: 0.5012\n",
      "Epoch [48/100], Step [0/18], Loss: 0.4631\n",
      "Epoch [48/100], Step [5/18], Loss: 0.3683\n",
      "Epoch [48/100], Step [10/18], Loss: 0.5709\n",
      "Epoch [48/100], Step [15/18], Loss: 0.3538\n",
      "Epoch [48/100], Loss: 0.5117\n",
      "Epoch [49/100], Step [0/18], Loss: 0.6465\n",
      "Epoch [49/100], Step [5/18], Loss: 0.4397\n",
      "Epoch [49/100], Step [10/18], Loss: 0.5484\n",
      "Epoch [49/100], Step [15/18], Loss: 0.5289\n",
      "Epoch [49/100], Loss: 0.4967\n",
      "Epoch [50/100], Step [0/18], Loss: 0.5885\n",
      "Epoch [50/100], Step [5/18], Loss: 0.4104\n",
      "Epoch [50/100], Step [10/18], Loss: 0.5218\n",
      "Epoch [50/100], Step [15/18], Loss: 0.7312\n",
      "Epoch [50/100], Loss: 0.4945\n",
      "Epoch [51/100], Step [0/18], Loss: 0.4862\n",
      "Epoch [51/100], Step [5/18], Loss: 0.5036\n",
      "Epoch [51/100], Step [10/18], Loss: 0.4912\n",
      "Epoch [51/100], Step [15/18], Loss: 0.3348\n",
      "Epoch [51/100], Loss: 0.4904\n",
      "Epoch [52/100], Step [0/18], Loss: 0.4553\n",
      "Epoch [52/100], Step [5/18], Loss: 0.5293\n",
      "Epoch [52/100], Step [10/18], Loss: 0.6240\n",
      "Epoch [52/100], Step [15/18], Loss: 0.5161\n",
      "Epoch [52/100], Loss: 0.4875\n",
      "Epoch [53/100], Step [0/18], Loss: 0.3900\n",
      "Epoch [53/100], Step [5/18], Loss: 0.6673\n",
      "Epoch [53/100], Step [10/18], Loss: 0.5643\n",
      "Epoch [53/100], Step [15/18], Loss: 0.4616\n",
      "Epoch [53/100], Loss: 0.4846\n",
      "Epoch [54/100], Step [0/18], Loss: 0.4454\n",
      "Epoch [54/100], Step [5/18], Loss: 0.4916\n",
      "Epoch [54/100], Step [10/18], Loss: 0.4302\n",
      "Epoch [54/100], Step [15/18], Loss: 0.4735\n",
      "Epoch [54/100], Loss: 0.4824\n",
      "Epoch [55/100], Step [0/18], Loss: 0.3936\n",
      "Epoch [55/100], Step [5/18], Loss: 0.3865\n",
      "Epoch [55/100], Step [10/18], Loss: 0.3869\n",
      "Epoch [55/100], Step [15/18], Loss: 0.4172\n",
      "Epoch [55/100], Loss: 0.4832\n",
      "Epoch [56/100], Step [0/18], Loss: 0.3760\n",
      "Epoch [56/100], Step [5/18], Loss: 0.3523\n",
      "Epoch [56/100], Step [10/18], Loss: 0.4173\n",
      "Epoch [56/100], Step [15/18], Loss: 0.4635\n",
      "Epoch [56/100], Loss: 0.4813\n",
      "Epoch [57/100], Step [0/18], Loss: 0.5728\n",
      "Epoch [57/100], Step [5/18], Loss: 0.6307\n",
      "Epoch [57/100], Step [10/18], Loss: 0.5883\n",
      "Epoch [57/100], Step [15/18], Loss: 0.3691\n",
      "Epoch [57/100], Loss: 0.4733\n",
      "Epoch [58/100], Step [0/18], Loss: 0.3273\n",
      "Epoch [58/100], Step [5/18], Loss: 0.4764\n",
      "Epoch [58/100], Step [10/18], Loss: 0.3994\n",
      "Epoch [58/100], Step [15/18], Loss: 0.5460\n",
      "Epoch [58/100], Loss: 0.4722\n",
      "Epoch [59/100], Step [0/18], Loss: 0.5841\n",
      "Epoch [59/100], Step [5/18], Loss: 0.5566\n",
      "Epoch [59/100], Step [10/18], Loss: 0.3334\n",
      "Epoch [59/100], Step [15/18], Loss: 0.4441\n",
      "Epoch [59/100], Loss: 0.4701\n",
      "Epoch [60/100], Step [0/18], Loss: 0.4053\n",
      "Epoch [60/100], Step [5/18], Loss: 0.4300\n",
      "Epoch [60/100], Step [10/18], Loss: 0.4226\n",
      "Epoch [60/100], Step [15/18], Loss: 0.4397\n",
      "Epoch [60/100], Loss: 0.4647\n",
      "Epoch [61/100], Step [0/18], Loss: 0.3903\n",
      "Epoch [61/100], Step [5/18], Loss: 0.5858\n",
      "Epoch [61/100], Step [10/18], Loss: 0.3194\n",
      "Epoch [61/100], Step [15/18], Loss: 0.3501\n",
      "Epoch [61/100], Loss: 0.4655\n",
      "Epoch [62/100], Step [0/18], Loss: 0.4790\n",
      "Epoch [62/100], Step [5/18], Loss: 0.3879\n",
      "Epoch [62/100], Step [10/18], Loss: 0.4527\n",
      "Epoch [62/100], Step [15/18], Loss: 0.4153\n",
      "Epoch [62/100], Loss: 0.4625\n",
      "Epoch [63/100], Step [0/18], Loss: 0.4489\n",
      "Epoch [63/100], Step [5/18], Loss: 0.4751\n",
      "Epoch [63/100], Step [10/18], Loss: 0.3574\n",
      "Epoch [63/100], Step [15/18], Loss: 0.3686\n",
      "Epoch [63/100], Loss: 0.4590\n",
      "Epoch [64/100], Step [0/18], Loss: 0.3423\n",
      "Epoch [64/100], Step [5/18], Loss: 0.3084\n",
      "Epoch [64/100], Step [10/18], Loss: 0.5757\n",
      "Epoch [64/100], Step [15/18], Loss: 0.3019\n",
      "Epoch [64/100], Loss: 0.4580\n",
      "Epoch [65/100], Step [0/18], Loss: 0.3166\n",
      "Epoch [65/100], Step [5/18], Loss: 0.3136\n",
      "Epoch [65/100], Step [10/18], Loss: 0.6068\n",
      "Epoch [65/100], Step [15/18], Loss: 0.3802\n",
      "Epoch [65/100], Loss: 0.4579\n",
      "Epoch [66/100], Step [0/18], Loss: 0.4491\n",
      "Epoch [66/100], Step [5/18], Loss: 0.3580\n",
      "Epoch [66/100], Step [10/18], Loss: 0.3603\n",
      "Epoch [66/100], Step [15/18], Loss: 0.3727\n",
      "Epoch [66/100], Loss: 0.4544\n",
      "Epoch [67/100], Step [0/18], Loss: 0.6602\n",
      "Epoch [67/100], Step [5/18], Loss: 0.6476\n",
      "Epoch [67/100], Step [10/18], Loss: 0.3645\n",
      "Epoch [67/100], Step [15/18], Loss: 0.2979\n",
      "Epoch [67/100], Loss: 0.4505\n",
      "Epoch [68/100], Step [0/18], Loss: 0.4893\n",
      "Epoch [68/100], Step [5/18], Loss: 0.3293\n",
      "Epoch [68/100], Step [10/18], Loss: 0.3141\n",
      "Epoch [68/100], Step [15/18], Loss: 0.3410\n",
      "Epoch [68/100], Loss: 0.4471\n",
      "Epoch [69/100], Step [0/18], Loss: 0.3130\n",
      "Epoch [69/100], Step [5/18], Loss: 0.5762\n",
      "Epoch [69/100], Step [10/18], Loss: 0.4125\n",
      "Epoch [69/100], Step [15/18], Loss: 0.3614\n",
      "Epoch [69/100], Loss: 0.4477\n",
      "Epoch [70/100], Step [0/18], Loss: 0.3835\n",
      "Epoch [70/100], Step [5/18], Loss: 0.3208\n",
      "Epoch [70/100], Step [10/18], Loss: 0.7190\n",
      "Epoch [70/100], Step [15/18], Loss: 0.4122\n",
      "Epoch [70/100], Loss: 0.4434\n",
      "Epoch [71/100], Step [0/18], Loss: 0.4547\n",
      "Epoch [71/100], Step [5/18], Loss: 0.3266\n",
      "Epoch [71/100], Step [10/18], Loss: 0.3528\n",
      "Epoch [71/100], Step [15/18], Loss: 0.4421\n",
      "Epoch [71/100], Loss: 0.4452\n",
      "Epoch [72/100], Step [0/18], Loss: 0.3453\n",
      "Epoch [72/100], Step [5/18], Loss: 0.4637\n",
      "Epoch [72/100], Step [10/18], Loss: 0.5152\n",
      "Epoch [72/100], Step [15/18], Loss: 0.2584\n",
      "Epoch [72/100], Loss: 0.4398\n",
      "Epoch [73/100], Step [0/18], Loss: 0.4100\n",
      "Epoch [73/100], Step [5/18], Loss: 0.3389\n",
      "Epoch [73/100], Step [10/18], Loss: 0.4635\n",
      "Epoch [73/100], Step [15/18], Loss: 0.3140\n",
      "Epoch [73/100], Loss: 0.4500\n",
      "Epoch [74/100], Step [0/18], Loss: 0.5518\n",
      "Epoch [74/100], Step [5/18], Loss: 0.4146\n",
      "Epoch [74/100], Step [10/18], Loss: 0.3647\n",
      "Epoch [74/100], Step [15/18], Loss: 0.3309\n",
      "Epoch [74/100], Loss: 0.4388\n",
      "Epoch [75/100], Step [0/18], Loss: 0.3373\n",
      "Epoch [75/100], Step [5/18], Loss: 0.4274\n",
      "Epoch [75/100], Step [10/18], Loss: 0.3087\n",
      "Epoch [75/100], Step [15/18], Loss: 0.5810\n",
      "Epoch [75/100], Loss: 0.4377\n",
      "Epoch [76/100], Step [0/18], Loss: 0.4415\n",
      "Epoch [76/100], Step [5/18], Loss: 0.3333\n",
      "Epoch [76/100], Step [10/18], Loss: 0.3857\n",
      "Epoch [76/100], Step [15/18], Loss: 0.5461\n",
      "Epoch [76/100], Loss: 0.4312\n",
      "Epoch [77/100], Step [0/18], Loss: 0.5701\n",
      "Epoch [77/100], Step [5/18], Loss: 0.3189\n",
      "Epoch [77/100], Step [10/18], Loss: 0.4870\n",
      "Epoch [77/100], Step [15/18], Loss: 0.3088\n",
      "Epoch [77/100], Loss: 0.4435\n",
      "Epoch [78/100], Step [0/18], Loss: 0.3478\n",
      "Epoch [78/100], Step [5/18], Loss: 0.4266\n",
      "Epoch [78/100], Step [10/18], Loss: 0.5068\n",
      "Epoch [78/100], Step [15/18], Loss: 0.6080\n",
      "Epoch [78/100], Loss: 0.4302\n",
      "Epoch [79/100], Step [0/18], Loss: 0.5207\n",
      "Epoch [79/100], Step [5/18], Loss: 0.3596\n",
      "Epoch [79/100], Step [10/18], Loss: 0.7527\n",
      "Epoch [79/100], Step [15/18], Loss: 0.2877\n",
      "Epoch [79/100], Loss: 0.4212\n",
      "Epoch [80/100], Step [0/18], Loss: 0.4096\n",
      "Epoch [80/100], Step [5/18], Loss: 0.3059\n",
      "Epoch [80/100], Step [10/18], Loss: 0.2351\n",
      "Epoch [80/100], Step [15/18], Loss: 0.4204\n",
      "Epoch [80/100], Loss: 0.4255\n",
      "Epoch [81/100], Step [0/18], Loss: 0.3591\n",
      "Epoch [81/100], Step [5/18], Loss: 0.2822\n",
      "Epoch [81/100], Step [10/18], Loss: 0.5569\n",
      "Epoch [81/100], Step [15/18], Loss: 0.4620\n",
      "Epoch [81/100], Loss: 0.4214\n",
      "Epoch [82/100], Step [0/18], Loss: 0.3772\n",
      "Epoch [82/100], Step [5/18], Loss: 0.4638\n",
      "Epoch [82/100], Step [10/18], Loss: 0.5442\n",
      "Epoch [82/100], Step [15/18], Loss: 0.3631\n",
      "Epoch [82/100], Loss: 0.4216\n",
      "Epoch [83/100], Step [0/18], Loss: 0.6750\n",
      "Epoch [83/100], Step [5/18], Loss: 0.3679\n",
      "Epoch [83/100], Step [10/18], Loss: 0.4315\n",
      "Epoch [83/100], Step [15/18], Loss: 0.5871\n",
      "Epoch [83/100], Loss: 0.4162\n",
      "Epoch [84/100], Step [0/18], Loss: 0.4061\n",
      "Epoch [84/100], Step [5/18], Loss: 0.3987\n",
      "Epoch [84/100], Step [10/18], Loss: 0.4187\n",
      "Epoch [84/100], Step [15/18], Loss: 0.4239\n",
      "Epoch [84/100], Loss: 0.4155\n",
      "Epoch [85/100], Step [0/18], Loss: 0.2768\n",
      "Epoch [85/100], Step [5/18], Loss: 0.2535\n",
      "Epoch [85/100], Step [10/18], Loss: 0.2658\n",
      "Epoch [85/100], Step [15/18], Loss: 0.3515\n",
      "Epoch [85/100], Loss: 0.4171\n",
      "Epoch [86/100], Step [0/18], Loss: 0.2581\n",
      "Epoch [86/100], Step [5/18], Loss: 0.3953\n",
      "Epoch [86/100], Step [10/18], Loss: 0.6281\n",
      "Epoch [86/100], Step [15/18], Loss: 0.2926\n",
      "Epoch [86/100], Loss: 0.4097\n",
      "Epoch [87/100], Step [0/18], Loss: 0.3592\n",
      "Epoch [87/100], Step [5/18], Loss: 0.3663\n",
      "Epoch [87/100], Step [10/18], Loss: 0.6042\n",
      "Epoch [87/100], Step [15/18], Loss: 0.4801\n",
      "Epoch [87/100], Loss: 0.4090\n",
      "Epoch [88/100], Step [0/18], Loss: 0.2833\n",
      "Epoch [88/100], Step [5/18], Loss: 0.3878\n",
      "Epoch [88/100], Step [10/18], Loss: 0.5543\n",
      "Epoch [88/100], Step [15/18], Loss: 0.8017\n",
      "Epoch [88/100], Loss: 0.4062\n",
      "Epoch [89/100], Step [0/18], Loss: 0.4000\n",
      "Epoch [89/100], Step [5/18], Loss: 0.3966\n",
      "Epoch [89/100], Step [10/18], Loss: 0.5398\n",
      "Epoch [89/100], Step [15/18], Loss: 0.2854\n",
      "Epoch [89/100], Loss: 0.4072\n",
      "Epoch [90/100], Step [0/18], Loss: 0.4227\n",
      "Epoch [90/100], Step [5/18], Loss: 0.3148\n",
      "Epoch [90/100], Step [10/18], Loss: 0.4995\n",
      "Epoch [90/100], Step [15/18], Loss: 0.2407\n",
      "Epoch [90/100], Loss: 0.4033\n",
      "Epoch [91/100], Step [0/18], Loss: 0.6315\n",
      "Epoch [91/100], Step [5/18], Loss: 0.4341\n",
      "Epoch [91/100], Step [10/18], Loss: 0.2726\n",
      "Epoch [91/100], Step [15/18], Loss: 0.3325\n",
      "Epoch [91/100], Loss: 0.4049\n",
      "Epoch [92/100], Step [0/18], Loss: 0.2068\n",
      "Epoch [92/100], Step [5/18], Loss: 0.3604\n",
      "Epoch [92/100], Step [10/18], Loss: 0.3211\n",
      "Epoch [92/100], Step [15/18], Loss: 0.2415\n",
      "Epoch [92/100], Loss: 0.3980\n",
      "Epoch [93/100], Step [0/18], Loss: 0.4347\n",
      "Epoch [93/100], Step [5/18], Loss: 0.3009\n",
      "Epoch [93/100], Step [10/18], Loss: 0.3094\n",
      "Epoch [93/100], Step [15/18], Loss: 0.3783\n",
      "Epoch [93/100], Loss: 0.4012\n",
      "Epoch [94/100], Step [0/18], Loss: 0.5553\n",
      "Epoch [94/100], Step [5/18], Loss: 0.5351\n",
      "Epoch [94/100], Step [10/18], Loss: 0.4007\n",
      "Epoch [94/100], Step [15/18], Loss: 0.3996\n",
      "Epoch [94/100], Loss: 0.3950\n",
      "Epoch [95/100], Step [0/18], Loss: 0.4810\n",
      "Epoch [95/100], Step [5/18], Loss: 0.2577\n",
      "Epoch [95/100], Step [10/18], Loss: 0.4944\n",
      "Epoch [95/100], Step [15/18], Loss: 0.8684\n",
      "Epoch [95/100], Loss: 0.4052\n",
      "Epoch [96/100], Step [0/18], Loss: 0.4365\n",
      "Epoch [96/100], Step [5/18], Loss: 0.4248\n",
      "Epoch [96/100], Step [10/18], Loss: 0.3679\n",
      "Epoch [96/100], Step [15/18], Loss: 0.5894\n",
      "Epoch [96/100], Loss: 0.3965\n",
      "Epoch [97/100], Step [0/18], Loss: 0.5568\n",
      "Epoch [97/100], Step [5/18], Loss: 0.1912\n",
      "Epoch [97/100], Step [10/18], Loss: 0.4760\n",
      "Epoch [97/100], Step [15/18], Loss: 0.3629\n",
      "Epoch [97/100], Loss: 0.3937\n",
      "Epoch [98/100], Step [0/18], Loss: 0.1594\n",
      "Epoch [98/100], Step [5/18], Loss: 0.2829\n",
      "Epoch [98/100], Step [10/18], Loss: 0.2773\n",
      "Epoch [98/100], Step [15/18], Loss: 0.4252\n",
      "Epoch [98/100], Loss: 0.3890\n",
      "Epoch [99/100], Step [0/18], Loss: 0.2481\n",
      "Epoch [99/100], Step [5/18], Loss: 0.2141\n",
      "Epoch [99/100], Step [10/18], Loss: 0.4818\n",
      "Epoch [99/100], Step [15/18], Loss: 0.3498\n",
      "Epoch [99/100], Loss: 0.3873\n",
      "Epoch [100/100], Step [0/18], Loss: 0.6774\n",
      "Epoch [100/100], Step [5/18], Loss: 0.3005\n",
      "Epoch [100/100], Step [10/18], Loss: 0.3258\n",
      "Epoch [100/100], Step [15/18], Loss: 0.2877\n",
      "Epoch [100/100], Loss: 0.3853\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    classifier.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (frames, labels) in enumerate(train_dataloader):\n",
    "        frames = frames.to(device)  # [batch_size, frames_per_video, C, H, W]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = classifier(frames)  # [batch_size, num_classes]\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i}/{len(train_dataloader)}], Loss: {loss.item():.4f}')\n",
    "    if epoch%20 == 0 :\n",
    "        torch.save(classifier.state_dict(), 'deadlift_classifier.pth')\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/amine.chraibi/.local/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 62.50%\n",
      "Predicted: bad movement, Actual: good movement\n",
      "Predicted: bad movement, Actual: bad movement\n",
      "Predicted: good movement, Actual: good movement\n",
      "Predicted: good movement, Actual: good movement\n",
      "Predicted: bad movement, Actual: bad movement\n",
      "Predicted: good movement, Actual: good movement\n",
      "Predicted: good movement, Actual: bad movement\n",
      "Predicted: good movement, Actual: bad movement\n"
     ]
    }
   ],
   "source": [
    "classifier.load_state_dict(torch.load('deadlift_classifier.pth'))\n",
    "classifier.eval()\n",
    "\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for frames, labels in test_dataloader:\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = classifier(frames)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total * 100\n",
    "print(f'Accuracy on the test set: {accuracy:.2f}%')\n",
    "\n",
    "idx_to_label = {v: k for k, v in test_dataset.label_to_idx.items()}\n",
    "all_predictions_labels = [idx_to_label[pred] for pred in all_predictions]\n",
    "all_labels_labels = [idx_to_label[label] for label in all_labels]\n",
    "\n",
    "for pred, true_label in zip(all_predictions_labels, all_labels_labels):\n",
    "    print(f'Predicted: {pred}, Actual: {true_label}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
